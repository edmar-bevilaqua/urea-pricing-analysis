{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2394dad3",
   "metadata": {},
   "source": [
    "### A. **Preço da ureia + commodities correlatas (base central)**\n",
    "\n",
    "* **World Bank - Commodity Price Data (“Pink Sheet”)**: traz série **mensal** de preços de commodities, incluindo **fertilizantes (ureia)** e também **gás natural, petróleo, grãos, outros fertilizantes**, etc. Ótima para montar features consistentes e alinhadas em frequência.\n",
    "* (Alternativa) **IMF Primary Commodity Prices**: também tem fertilizantes e séries de referência.\n",
    "\n",
    "**Por que é “a base”**: com ela você já cobre direto vários itens da lista do cliente: gás, petróleo, grãos, nitrogenados substitutos e até proxies de energia.\n",
    "\n",
    "---\n",
    "\n",
    "### B. **Câmbio (para preço local e efeito de importação)**\n",
    "\n",
    "* **BCB PTAX (API OData)**: cotações diárias (compra/venda) e você agrega para mensal (média/último dia útil).\n",
    "\n",
    "---\n",
    "\n",
    "### C. **Fretes / logística (proxy robusta e mensal)**\n",
    "\n",
    "* **NY Fed - Global Supply Chain Pressure Index (GSCPI)**: índice mensal que incorpora custos de transporte (inclui medidas baseadas em frete marítimo como BDI/Harpex) e variáveis de oferta. Serve como proxy muito boa para “frete marítimo / gargalos”.\n",
    "\n",
    "---\n",
    "\n",
    "### D. **Geopolítica (guerras, sanções, tensões, tarifas)**\n",
    "\n",
    "* **Geopolitical Risk Index (GPR)** (Caldara & Iacoviello): série **mensal** amplamente usada como proxy quantitativa de risco geopolítico (guerras/tensão/sanções).\n",
    "* (Opcional) **Economic Policy Uncertainty (EPU)** via FRED para “política / tarifas / incerteza macro” (também mensal).\n",
    "\n",
    "---\n",
    "\n",
    "### E. **Clima (chuvas / ENSO como proxy global)**\n",
    "\n",
    "* **NOAA ONI (Oceanic Niño Index)**: série mensal em CSV (ENSO), boa proxy de variações climáticas com impacto em agricultura/demanda logística.\n",
    "---\n",
    "\n",
    "### F. **Trade flows (China exportação, Índia import/tenders) - opcional**\n",
    "\n",
    "* **UN Comtrade / WITS**: dá para extrair exportações/importações de ureia (ex.: China) e usar como feature (volume/valor), mas automatização pode exigir mais “engenharia”.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from io import BytesIO\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(\"All necessary libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb4e060",
   "metadata": {},
   "source": [
    "## Utilitários + Configs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e694dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD_BANK_PINK_SHEET_MONTHLY_XLSX = (\n",
    "    \"https://thedocs.worldbank.org/en/doc/18675f1d1639c7a34d463f59263ba0a2-0050012025/related/CMO-Historical-Data-Monthly.xlsx\"\n",
    ")\n",
    "\n",
    "NYFED_GSCPI_XLSX = (\n",
    "    \"https://www.newyorkfed.org/medialibrary/research/interactives/gscpi/downloads/gscpi_data.xlsx\"\n",
    ")\n",
    "\n",
    "NOAA_ONI_CSV = \"https://psl.noaa.gov/data/correlation/oni.csv\"\n",
    "\n",
    "GPR_XLS = \"https://www.matteoiacoviello.com/gpr_files/data_gpr_export.xls\"\n",
    "\n",
    "# BCB PTAX (OData) - exemplo comum com parâmetros:\n",
    "# https://olinda.bcb.gov.br/olinda/servico/PTAX/versao/v1/odata/CotacaoDolarPeriodo(dataInicial=@dataInicial,dataFinalCotacao=@dataFinalCotacao)?@dataInicial='01-01-2020'&@dataFinalCotacao='12-31-2050'&$format=json&$select=cotacaoCompra,cotacaoVenda,dataHoraCotacao\n",
    "BCB_PTAX_BASE = \"https://olinda.bcb.gov.br/olinda/servico/PTAX/versao/v1/odata\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SeriesSpec:\n",
    "    out_name: str\n",
    "    patterns: List[str]  # regex list to find a column in Pink Sheet\n",
    "\n",
    "\n",
    "# Tentei deixar genérico o suficiente para sobreviver a pequenas mudanças de header.\n",
    "PINK_SHEET_SERIES: List[SeriesSpec] = [\n",
    "    SeriesSpec(\"urea_usd\", [r\"\\burea\\b\"]),\n",
    "    SeriesSpec(\"natural_gas_usd\", [r\"natural\\s*gas\", r\"\\bng\\b\"]),\n",
    "    SeriesSpec(\"crude_oil_usd\", [r\"crude.*oil\", r\"\\bbrent\\b\", r\"\\bwt[i|l]\\b\"]),\n",
    "    SeriesSpec(\"maize_usd\", [r\"\\bmaize\\b\", r\"\\bcorn\\b\"]),\n",
    "    SeriesSpec(\"wheat_usd\", [r\"\\bwheat\\b\"]),\n",
    "    SeriesSpec(\"soybeans_usd\", [r\"\\bsoy\\b\", r\"\\bsoybeans?\\b\"]),\n",
    "    SeriesSpec(\"ammonia_usd\", [r\"\\bammonia\\b\"]),\n",
    "    SeriesSpec(\"dap_usd\", [r\"\\bdap\\b\", r\"diammonium\\s*phosphate\"]),\n",
    "    SeriesSpec(\"potassium_usd\", [r\"\\bpotassium\\b\"]),\n",
    "]\n",
    "\n",
    "\n",
    "def _safe_mkdir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def _http_get(url: str, timeout: int = 60) -> bytes:\n",
    "    r = requests.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "\n",
    "def _to_month_start(dt: pd.Series) -> pd.Series:\n",
    "    d = pd.to_datetime(dt, errors=\"coerce\")\n",
    "    return d.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "\n",
    "def _month_index(df: pd.DataFrame, date_col: str = \"date\") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[date_col] = _to_month_start(df[date_col])\n",
    "    df = df.dropna(subset=[date_col])\n",
    "    return df.set_index(date_col).sort_index()\n",
    "\n",
    "\n",
    "def _pick_best_column(columns: List[str], patterns: List[str]) -> Optional[str]:\n",
    "    cols_norm = {c: re.sub(r\"\\s+\", \" \", str(c)).strip().lower() for c in columns}\n",
    "    for pat in patterns:\n",
    "        rx = re.compile(pat, flags=re.IGNORECASE)\n",
    "        matches = [c for c, cn in cols_norm.items() if rx.search(cn)]\n",
    "        if len(matches) == 1:\n",
    "            return matches[0]\n",
    "        if len(matches) > 1:\n",
    "            # Heurística: se tiver \"urea\" e \"gulf\"/\"bulk\"/\"granular\", etc, escolha o mais descritivo\n",
    "            # Caso não, escolha o primeiro em ordem alfabética (estável).\n",
    "            matches_sorted = sorted(matches, key=lambda x: (len(str(x)), str(x)))\n",
    "            return matches_sorted[0]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2c2e3",
   "metadata": {},
   "source": [
    "## Loaders\n",
    "---\n",
    "Com cache local em `/data/raw/` para evitar baixar os arquivos sempre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pink_sheet_monthly(selected: List[SeriesSpec]) -> pd.DataFrame:\n",
    "    # Cache local\n",
    "    _safe_mkdir(os.path.join(\"data\", \"raw\"))\n",
    "    csv_path = os.path.join(\"data\", \"raw\", \"pink_sheet_monthly.csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "        return _month_index(df, \"date\")\n",
    "\n",
    "    # Buscar da web e salvar\n",
    "    content = _http_get(WORLD_BANK_PINK_SHEET_MONTHLY_XLSX)\n",
    "    xls = pd.ExcelFile(BytesIO(content))\n",
    "\n",
    "    # Normalmente o primeiro sheet já é o \"Monthly Prices\", mas deixamos robusto:\n",
    "    sheet_name = xls.sheet_names[1]\n",
    "    df_raw = pd.read_excel(xls, sheet_name=sheet_name, engine=\"openpyxl\", skiprows=4, header=[0, 1])\n",
    "\n",
    "    # Flatten MultiIndex columns: juntar nome e unidade com espaço\n",
    "    df_raw.columns = [' '.join(col).strip() for col in df_raw.columns.values]\n",
    "\n",
    "    # Descobrir coluna de data (alguns arquivos usam \"date\" / \"Month\" / \"Time\")\n",
    "    possible_date_cols = [c for c in df_raw.columns if str(c).strip().lower() in (\"date\", \"time\", \"month\")]\n",
    "    if not possible_date_cols:\n",
    "        # fallback: primeira coluna\n",
    "        date_col = df_raw.columns[0]\n",
    "    else:\n",
    "        date_col = possible_date_cols[0]\n",
    "\n",
    "    df = df_raw.rename(columns={date_col: \"date\"}).copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"].astype(str).str.replace('M', ''), format='%Y%m', errors='coerce')\n",
    "\n",
    "    # Selecionar séries por regex\n",
    "    picked = {}\n",
    "    missing = []\n",
    "    for spec in selected:\n",
    "        col = _pick_best_column(list(df.columns), spec.patterns)\n",
    "        if col is None:\n",
    "            missing.append(spec.out_name)\n",
    "            continue\n",
    "        picked[col] = spec.out_name\n",
    "\n",
    "    if missing:\n",
    "        print(\"[AVISO] Algumas séries não foram encontradas no Pink Sheet:\", missing)\n",
    "        print(\"[DEBUG] Colunas disponíveis (amostra):\", list(df.columns)[:30])\n",
    "\n",
    "    out = df[[\"date\"] + list(picked.keys())].rename(columns=picked)\n",
    "\n",
    "    # Converter tudo para numérico (algumas colunas podem vir como object)\n",
    "    for c in out.columns:\n",
    "        if c != \"date\":\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    out_idx = _month_index(out, \"date\")\n",
    "    out_idx.reset_index().to_csv(csv_path, index=False)\n",
    "    return out_idx\n",
    "\n",
    "\n",
    "def load_bcb_ptax_usdbrl(date_start: str, date_end: str) -> pd.DataFrame:\n",
    "    # A API usa formato dd-mm-aaaa nas strings do parâmetro\n",
    "    # Vamos aceitar start/end como \"YYYY-MM\" ou \"YYYY-MM-DD\" e converter.\n",
    "    start_dt = pd.to_datetime(date_start) if len(date_start) > 7 else pd.to_datetime(date_start + \"-01\")\n",
    "    end_dt = pd.to_datetime(date_end) if len(date_end) > 7 else (pd.to_datetime(date_end + \"-01\") + pd.offsets.MonthEnd(0))\n",
    "\n",
    "    start_str = start_dt.strftime(\"%m-%d-%Y\")  # muitos exemplos aceitam MM-DD-YYYY\n",
    "    end_str = end_dt.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "    url = (\n",
    "        f\"{BCB_PTAX_BASE}/CotacaoDolarPeriodo(dataInicial=@dataInicial,dataFinalCotacao=@dataFinalCotacao)\"\n",
    "        f\"?@dataInicial='{start_str}'&@dataFinalCotacao='{end_str}'&$format=json\"\n",
    "        f\"&$select=cotacaoCompra,cotacaoVenda,dataHoraCotacao\"\n",
    "    )\n",
    "\n",
    "    # Cache por janela de datas\n",
    "    _safe_mkdir(os.path.join(\"data\", \"raw\"))\n",
    "    safe_start = date_start.replace(\"/\", \"-\").replace(\" \", \"_\")\n",
    "    safe_end = date_end.replace(\"/\", \"-\").replace(\" \", \"_\")\n",
    "    csv_path = os.path.join(\"data\", \"raw\", f\"bcb_ptax_usdbrl_{safe_start}_{safe_end}.csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df_m = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "        return _month_index(df_m, \"date\")\n",
    "\n",
    "    data = _http_get(url)\n",
    "    j = json.loads(data.decode(\"utf-8\"))\n",
    "    values = j.get(\"value\", [])\n",
    "    df = pd.DataFrame(values)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"BCB PTAX: retorno vazio. Verifique janela de datas/URL.\")\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"dataHoraCotacao\"], errors=\"coerce\")\n",
    "    df[\"usdbrl\"] = pd.to_numeric(df[\"cotacaoVenda\"], errors=\"coerce\")\n",
    "\n",
    "    # Agregar para mensal (média)\n",
    "    df_m = (\n",
    "        df.dropna(subset=[\"date\", \"usdbrl\"])\n",
    "          .assign(date=_to_month_start(df[\"date\"]))\n",
    "          .groupby(\"date\", as_index=False)[\"usdbrl\"]\n",
    "          .mean()\n",
    "    )\n",
    "    df_m.reset_index(drop=True)\n",
    "    df_m.reset_index().to_csv(csv_path, index=False)\n",
    "    return _month_index(df_m, \"date\")\n",
    "\n",
    "\n",
    "def load_nyfed_gscpi() -> pd.DataFrame:\n",
    "    _safe_mkdir(os.path.join(\"data\", \"raw\"))\n",
    "    csv_path = os.path.join(\"data\", \"raw\", \"nyfed_gscpi.csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "        return _month_index(df, \"date\")\n",
    "\n",
    "    content = _http_get(NYFED_GSCPI_XLSX)\n",
    "    xls = pd.ExcelFile(BytesIO(content))\n",
    "    # Em geral há duas sheet com a série e coluna \"GSCPI\"\n",
    "    sheet = xls.sheet_names[1]\n",
    "    df = pd.read_excel(xls, sheet_name=sheet)\n",
    "\n",
    "    # Tentar inferir colunas:\n",
    "    date_col = _pick_best_column(list(df.columns), [r\"date\", r\"month\", r\"time\"]) or df.columns[0]\n",
    "    val_col = _pick_best_column(list(df.columns), [r\"gscpi\"]) or df.columns[1]\n",
    "\n",
    "    out = df.rename(columns={date_col: \"date\", val_col: \"gscpi\"})[[\"date\", \"gscpi\"]].copy()\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "    out[\"gscpi\"] = pd.to_numeric(out[\"gscpi\"], errors=\"coerce\")\n",
    "    out_idx = _month_index(out, \"date\")\n",
    "    out_idx.reset_index().to_csv(csv_path, index=False)\n",
    "    return out_idx\n",
    "\n",
    "\n",
    "def load_noaa_oni() -> pd.DataFrame:\n",
    "    _safe_mkdir(os.path.join(\"data\", \"raw\"))\n",
    "    csv_path = os.path.join(\"data\", \"raw\", \"noaa_oni.csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "        return _month_index(df, \"date\")\n",
    "\n",
    "    content = _http_get(NOAA_ONI_CSV)\n",
    "    df = pd.read_csv(BytesIO(content))\n",
    "    # Esperado: date, ONI\n",
    "    date_col = _pick_best_column(list(df.columns), [r\"date\"]) or df.columns[0]\n",
    "    val_col = _pick_best_column(list(df.columns), [r\"oni\"]) or df.columns[1]\n",
    "    out = df.rename(columns={date_col: \"date\", val_col: \"oni\"})[[\"date\", \"oni\"]].copy()\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "    out[\"oni\"] = pd.to_numeric(out[\"oni\"], errors=\"coerce\")\n",
    "    out_idx = _month_index(out, \"date\")\n",
    "    out_idx.reset_index().to_csv(csv_path, index=False)\n",
    "    return out_idx\n",
    "\n",
    "\n",
    "def load_gpr() -> pd.DataFrame:\n",
    "    _safe_mkdir(os.path.join(\"data\", \"raw\"))\n",
    "    csv_path = os.path.join(\"data\", \"raw\", \"gpr.csv\")\n",
    "    gpr_dict_path = os.path.join(\"data\", \"raw\", \"gpr_dict.csv\")\n",
    "    if os.path.exists(csv_path) and os.path.exists(gpr_dict_path):\n",
    "        df = pd.read_csv(csv_path, parse_dates=[\"date\"]).set_index(\"date\")\n",
    "        dictionary = pd.read_csv(gpr_dict_path)\n",
    "        return (_month_index(df.reset_index(), \"date\"), dictionary)\n",
    "\n",
    "    content = _http_get(GPR_XLS)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(BytesIO(content), engine=\"xlrd\")\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"Para ler arquivos .xls, instale xlrd: pip install xlrd\"\n",
    "        ) from e\n",
    "    except ValueError:\n",
    "        # fallback: tenta sem engine explicita\n",
    "        df = pd.read_excel(BytesIO(content))\n",
    "\n",
    "    # Normaliza nomes\n",
    "    cols_norm = {c: re.sub(r\"\\s+\", \" \", str(c)).strip().lower() for c in df.columns}\n",
    "\n",
    "    # --- 1) Construir coluna date ---\n",
    "    # Caso A: tem year e month separados\n",
    "    year_col = next((c for c, cn in cols_norm.items() if cn in (\"year\", \"yr\", \"yyyy\")), None)\n",
    "    month_col = next((c for c, cn in cols_norm.items() if cn in (\"month\", \"mo\", \"mm\")), None)\n",
    "\n",
    "    if year_col is not None and month_col is not None:\n",
    "        out = df[[year_col, month_col]].copy()\n",
    "        out[\"date\"] = pd.to_datetime(\n",
    "            dict(year=pd.to_numeric(out[year_col], errors=\"coerce\"),\n",
    "                 month=pd.to_numeric(out[month_col], errors=\"coerce\"),\n",
    "                 day=1),\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "        df2 = df.copy()\n",
    "        df2[\"date\"] = out[\"date\"]\n",
    "    else:\n",
    "        # Caso B: tem date / time / month como coluna única\n",
    "        date_col = _pick_best_column(list(df.columns), [r\"date\", r\"time\", r\"month\"]) or df.columns[0]\n",
    "        df2 = df.rename(columns={date_col: \"date\"}).copy()\n",
    "        # Se vier como YYYYMM numérico (ex.: 202112), converte\n",
    "        if pd.api.types.is_numeric_dtype(df2[\"date\"]):\n",
    "            yyyymm = pd.to_numeric(df2[\"date\"], errors=\"coerce\")\n",
    "            year = (yyyymm // 100).astype(\"Int64\")\n",
    "            month = (yyyymm % 100).astype(\"Int64\")\n",
    "            df2[\"date\"] = pd.to_datetime(dict(year=year, month=month, day=1), errors=\"coerce\")\n",
    "        else:\n",
    "            df2[\"date\"] = pd.to_datetime(df2[\"date\"], errors=\"coerce\")\n",
    "    \n",
    "    dictionary = pd.DataFrame()\n",
    "    if os.path.exists(gpr_dict_path):\n",
    "        dictionary = pd.read_csv(gpr_dict_path, parse_dates=[\"date\"]).set_index(\"date\")\n",
    "\n",
    "    if dictionary.empty:\n",
    "        dictionary = df2[[\"var_name\", \"var_label\"]].dropna().copy()\n",
    "        dictionary.loc[0, \"var_name\"] = \"date\"\n",
    "        dictionary[\"var_name\"] = dictionary[\"var_name\"].apply(str.lower)\n",
    "        dictionary.to_csv(gpr_dict_path, index=False)\n",
    "\n",
    "    # --- 2) Selecionar colunas do GPR ---\n",
    "    # Ele pode ter GPR agregado e também decomposições (ameaça/ato etc.). Vamos puxar tudo que começar com gpr\n",
    "    gpr_cols = [c for c, cn in cols_norm.items() if cn == \"gpr\" or cn.startswith(\"gpr\")]\n",
    "\n",
    "    if not gpr_cols:\n",
    "        # fallback: tenta achar pelo regex no nome original\n",
    "        gpr_cols = [c for c in df2.columns if re.search(r\"(^|\\W)gpr(\\W|$)\", str(c), flags=re.IGNORECASE)]\n",
    "\n",
    "    if not gpr_cols:\n",
    "        raise RuntimeError(\"Não encontrei nenhuma coluna 'GPR' no arquivo data_gpr_export.xls.\")\n",
    "\n",
    "    keep = [\"date\"] + gpr_cols\n",
    "    out = df2[keep].copy()\n",
    "\n",
    "    # Numeric\n",
    "    for c in gpr_cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    # Padroniza nomes (opcional)\n",
    "    rename_map = {}\n",
    "    for c in gpr_cols:\n",
    "        cn = cols_norm.get(c, str(c).lower())\n",
    "        # deixa nomes amigáveis\n",
    "        rename_map[c] = re.sub(r\"[^a-z0-9_]+\", \"_\", cn).strip(\"_\")\n",
    "    out = out.rename(columns=rename_map)\n",
    "\n",
    "    out_idx = _month_index(out, \"date\")\n",
    "    out_idx.reset_index().to_csv(csv_path, index=False)\n",
    "    return (out_idx, dictionary)\n",
    "\n",
    "def load_events():\n",
    "    _safe_mkdir(os.path.join(\"data\", \"raw\"))\n",
    "    csv_path = os.path.join(\"data\", \"raw\", \"main_events.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"date\"] = df['period'].str[:7]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m\", errors=\"coerce\")\n",
    "    df = df[[\"period\", \"date\", \"event\"]]\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return _month_index(df, \"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8744dbe",
   "metadata": {},
   "source": [
    "## Definindo os parâmetros:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = os.path.abspath(\"output\")\n",
    "DATA_DIR = os.path.abspath(\"data\")\n",
    "START_DATE = \"1990-01\"\n",
    "END_DATE = \"2025-12\"\n",
    "TARGET = \"Urea  ($/mt)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61986102",
   "metadata": {},
   "source": [
    "## Executando os loaders:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_safe_mkdir(OUTDIR)\n",
    "figdir = os.path.join(OUTDIR, \"figures\")\n",
    "_safe_mkdir(figdir)\n",
    "\n",
    "# 1) Loaders\n",
    "print(\"Baixando Pink Sheet (World Bank)...\")\n",
    "df_prices = load_pink_sheet_monthly(PINK_SHEET_SERIES)\n",
    "\n",
    "print(\"Baixando câmbio PTAX (BCB)...\")\n",
    "df_fx = load_bcb_ptax_usdbrl(START_DATE, END_DATE)\n",
    "\n",
    "print(\"Baixando GSCPI (NY Fed)...\")\n",
    "df_gscpi = load_nyfed_gscpi()\n",
    "\n",
    "print(\"Baixando GPR (Geopolitical Risk)...\")\n",
    "df_gpr, gpr_dict = load_gpr()\n",
    "\n",
    "print(\"Baixando ONI (NOAA)...\")\n",
    "df_oni = load_noaa_oni()\n",
    "\n",
    "print(\"Carregando eventos principais...\")\n",
    "df_events = load_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47be24f",
   "metadata": {},
   "source": [
    "## Unificando todas as tabelas em uma só:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2100cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpr_small = (\n",
    "    df_gpr[\n",
    "        [\n",
    "            \"gpr\",\n",
    "            \"gprt\",\n",
    "            \"gpra\",\n",
    "            \"gprh\",\n",
    "            \"gprht\",\n",
    "            \"gprha\",\n",
    "            \"gprc_bra\",\n",
    "            \"gprhc_bra\",\n",
    "            \"gprc_chn\",\n",
    "            \"gprhc_chn\",\n",
    "            \"gprc_ind\",\n",
    "            \"gprhc_ind\"\n",
    "        ]\n",
    "    ].copy()\n",
    ")\n",
    "df = (\n",
    "    df_prices\n",
    "    .merge(df_fx, how=\"outer\", left_index=True, right_index=True)\n",
    "    .merge(df_gscpi, how=\"outer\", left_index=True, right_index=True)\n",
    "    .merge(df_gpr_small, how=\"outer\", left_index=True, right_index=True)\n",
    "    .merge(df_oni, how=\"outer\", left_index=True, right_index=True)\n",
    "    .merge(df_events, how=\"outer\", left_index=True, right_index=True)\n",
    ")\n",
    "df = df.loc[(df.index >= pd.to_datetime(START_DATE + \"-01\" if len(START_DATE) == 7 else START_DATE)) &\n",
    "            (df.index <= pd.to_datetime(END_DATE + \"-01\" if len(END_DATE) == 7 else END_DATE) + pd.offsets.MonthEnd(0))]\n",
    "\n",
    "df.to_csv(os.path.join(DATA_DIR, \"merged_data.csv\"), index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urea-pricing-analysis-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
