{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2394dad3",
   "metadata": {},
   "source": [
    "### A. **Preço da ureia + commodities correlatas (base central)**\n",
    "\n",
    "* **World Bank - Commodity Price Data (“Pink Sheet”)**: traz série **mensal** de preços de commodities, incluindo **fertilizantes (ureia)** e também **gás natural, petróleo, grãos, outros fertilizantes**, etc. Ótima para montar features consistentes e alinhadas em frequência.\n",
    "* (Alternativa) **IMF Primary Commodity Prices**: também tem fertilizantes e séries de referência.\n",
    "\n",
    "**Por que é “a base”**: com ela você já cobre direto vários itens da lista do cliente: gás, petróleo, grãos, nitrogenados substitutos e até proxies de energia.\n",
    "\n",
    "---\n",
    "\n",
    "### B. **Câmbio (para preço local e efeito de importação)**\n",
    "\n",
    "* **BCB PTAX (API OData)**: cotações diárias (compra/venda) e você agrega para mensal (média/último dia útil).\n",
    "\n",
    "---\n",
    "\n",
    "### C. **Fretes / logística (proxy robusta e mensal)**\n",
    "\n",
    "* **NY Fed - Global Supply Chain Pressure Index (GSCPI)**: índice mensal que incorpora custos de transporte (inclui medidas baseadas em frete marítimo como BDI/Harpex) e variáveis de oferta. Serve como proxy muito boa para “frete marítimo / gargalos”.\n",
    "\n",
    "---\n",
    "\n",
    "### D. **Geopolítica (guerras, sanções, tensões, tarifas)**\n",
    "\n",
    "* **Geopolitical Risk Index (GPR)** (Caldara & Iacoviello): série **mensal** amplamente usada como proxy quantitativa de risco geopolítico (guerras/tensão/sanções).\n",
    "* (Opcional) **Economic Policy Uncertainty (EPU)** via FRED para “política / tarifas / incerteza macro” (também mensal).\n",
    "\n",
    "---\n",
    "\n",
    "### E. **Clima (chuvas / ENSO como proxy global)**\n",
    "\n",
    "* **NOAA ONI (Oceanic Niño Index)**: série mensal em CSV (ENSO), boa proxy de variações climáticas com impacto em agricultura/demanda logística.\n",
    "---\n",
    "\n",
    "### F. **Trade flows (China exportação, Índia import/tenders) - opcional**\n",
    "\n",
    "* **UN Comtrade / WITS**: dá para extrair exportações/importações de ureia (ex.: China) e usar como feature (volume/valor), mas automatização pode exigir mais “engenharia”.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from io import BytesIO\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e694dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Config / utilitários\n",
    "# =========================\n",
    "\n",
    "WORLD_BANK_PINK_SHEET_MONTHLY_XLSX = (\n",
    "    \"https://thedocs.worldbank.org/en/doc/5d903e848db1d1b83e0ec8f744e55570-0350012021/related/CMO-Historical-Data-Monthly.xlsx\"\n",
    ")\n",
    "\n",
    "NYFED_GSCPI_XLSX = (\n",
    "    \"https://www.newyorkfed.org/medialibrary/research/interactives/gscpi/downloads/gscpi_data.xlsx\"\n",
    ")\n",
    "\n",
    "NOAA_ONI_CSV = \"https://psl.noaa.gov/data/correlation/oni.csv\"\n",
    "\n",
    "GPR_XLS = \"https://www.matteoiacoviello.com/gpr_files/data_gpr_export.xls\"\n",
    "\n",
    "# BCB PTAX (OData) - exemplo comum com parâmetros:\n",
    "# https://olinda.bcb.gov.br/olinda/servico/PTAX/versao/v1/odata/CotacaoDolarPeriodo(dataInicial=@dataInicial,dataFinalCotacao=@dataFinalCotacao)?@dataInicial='01-01-2020'&@dataFinalCotacao='12-31-2050'&$format=json&$select=cotacaoCompra,cotacaoVenda,dataHoraCotacao\n",
    "BCB_PTAX_BASE = \"https://olinda.bcb.gov.br/olinda/servico/PTAX/versao/v1/odata\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SeriesSpec:\n",
    "    out_name: str\n",
    "    patterns: List[str]  # regex list to find a column in Pink Sheet\n",
    "\n",
    "\n",
    "# Tentei deixar genérico o suficiente para sobreviver a pequenas mudanças de header.\n",
    "PINK_SHEET_SERIES: List[SeriesSpec] = [\n",
    "    SeriesSpec(\"urea_usd\", [r\"\\burea\\b\"]),\n",
    "    SeriesSpec(\"natural_gas_usd\", [r\"natural\\s*gas\", r\"\\bng\\b\"]),\n",
    "    SeriesSpec(\"crude_oil_usd\", [r\"crude.*oil\", r\"\\bbrent\\b\", r\"\\bwt[i|l]\\b\"]),\n",
    "    SeriesSpec(\"maize_usd\", [r\"\\bmaize\\b\", r\"\\bcorn\\b\"]),\n",
    "    SeriesSpec(\"wheat_usd\", [r\"\\bwheat\\b\"]),\n",
    "    SeriesSpec(\"soybeans_usd\", [r\"\\bsoy\\b\", r\"\\bsoybeans?\\b\"]),\n",
    "    SeriesSpec(\"ammonia_usd\", [r\"\\bammonia\\b\"]),\n",
    "    SeriesSpec(\"dap_usd\", [r\"\\bdap\\b\", r\"diammonium\\s*phosphate\"]),\n",
    "    SeriesSpec(\"potassium_usd\", [r\"\\bpotassium\\b\"]),\n",
    "]\n",
    "\n",
    "\n",
    "def _safe_mkdir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def _http_get(url: str, timeout: int = 60) -> bytes:\n",
    "    r = requests.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "\n",
    "def _to_month_start(dt: pd.Series) -> pd.Series:\n",
    "    d = pd.to_datetime(dt, errors=\"coerce\")\n",
    "    return d.dt.to_period(\"M\").dt.to_timestamp(how=\"start\")\n",
    "\n",
    "\n",
    "def _month_index(df: pd.DataFrame, date_col: str = \"date\") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[date_col] = _to_month_start(df[date_col])\n",
    "    df = df.dropna(subset=[date_col])\n",
    "    return df.set_index(date_col).sort_index()\n",
    "\n",
    "\n",
    "def _pick_best_column(columns: List[str], patterns: List[str]) -> Optional[str]:\n",
    "    cols_norm = {c: re.sub(r\"\\s+\", \" \", str(c)).strip().lower() for c in columns}\n",
    "    for pat in patterns:\n",
    "        rx = re.compile(pat, flags=re.IGNORECASE)\n",
    "        matches = [c for c, cn in cols_norm.items() if rx.search(cn)]\n",
    "        if len(matches) == 1:\n",
    "            return matches[0]\n",
    "        if len(matches) > 1:\n",
    "            # Heurística: se tiver \"urea\" e \"gulf\"/\"bulk\"/\"granular\", etc, escolha o mais descritivo\n",
    "            # Caso não, escolha o primeiro em ordem alfabética (estável).\n",
    "            matches_sorted = sorted(matches, key=lambda x: (len(str(x)), str(x)))\n",
    "            return matches_sorted[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Loaders\n",
    "# =========================\n",
    "\n",
    "def load_pink_sheet_monthly(selected: List[SeriesSpec]) -> pd.DataFrame:\n",
    "    content = _http_get(WORLD_BANK_PINK_SHEET_MONTHLY_XLSX)\n",
    "    xls = pd.ExcelFile(BytesIO(content))\n",
    "\n",
    "    # Normalmente o primeiro sheet já é o \"Monthly Prices\", mas deixamos robusto:\n",
    "    sheet_name = xls.sheet_names[1]\n",
    "    df_raw = pd.read_excel(xls, sheet_name=sheet_name, engine=\"openpyxl\", skiprows=4, header=[0, 1])\n",
    "\n",
    "    # Flatten MultiIndex columns: juntar nome e unidade com espaço\n",
    "    df_raw.columns = [' '.join(col).strip() for col in df_raw.columns.values]\n",
    "\n",
    "    # Descobrir coluna de data (alguns arquivos usam \"date\" / \"Month\" / \"Time\")\n",
    "    possible_date_cols = [c for c in df_raw.columns if str(c).strip().lower() in (\"date\", \"time\", \"month\")]\n",
    "    if not possible_date_cols:\n",
    "        # fallback: primeira coluna\n",
    "        date_col = df_raw.columns[0]\n",
    "    else:\n",
    "        date_col = possible_date_cols[0]\n",
    "\n",
    "    df = df_raw.rename(columns={date_col: \"date\"}).copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"].str.replace('M', ''), format='%Y%m', errors='coerce')\n",
    "\n",
    "    # Selecionar séries por regex\n",
    "    picked = {}\n",
    "    missing = []\n",
    "    for spec in selected:\n",
    "        col = _pick_best_column(list(df.columns), spec.patterns)\n",
    "        if col is None:\n",
    "            missing.append(spec.out_name)\n",
    "            continue\n",
    "        picked[spec.out_name] = col\n",
    "\n",
    "    if missing:\n",
    "        print(\"[AVISO] Algumas séries não foram encontradas no Pink Sheet:\", missing)\n",
    "        # Ajuda a ajustar rapidamente:\n",
    "        print(\"[DEBUG] Colunas disponíveis (amostra):\", list(df.columns)[:30])\n",
    "\n",
    "    out = df[[\"date\"] + list(picked.values())].rename(columns=picked)\n",
    "\n",
    "    # Converter tudo para numérico (algumas colunas podem vir como object)\n",
    "    for c in out.columns:\n",
    "        if c != \"date\":\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    return _month_index(out, \"date\")\n",
    "\n",
    "\n",
    "def load_bcb_ptax_usdbrl(date_start: str, date_end: str) -> pd.DataFrame:\n",
    "    # A API usa formato dd-mm-aaaa nas strings do parâmetro\n",
    "    # Vamos aceitar start/end como \"YYYY-MM\" ou \"YYYY-MM-DD\" e converter.\n",
    "    start_dt = pd.to_datetime(date_start) if len(date_start) > 7 else pd.to_datetime(date_start + \"-01\")\n",
    "    end_dt = pd.to_datetime(date_end) if len(date_end) > 7 else (pd.to_datetime(date_end + \"-01\") + pd.offsets.MonthEnd(0))\n",
    "\n",
    "    start_str = start_dt.strftime(\"%m-%d-%Y\")  # muitos exemplos aceitam MM-DD-YYYY\n",
    "    end_str = end_dt.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "    url = (\n",
    "        f\"{BCB_PTAX_BASE}/CotacaoDolarPeriodo(dataInicial=@dataInicial,dataFinalCotacao=@dataFinalCotacao)\"\n",
    "        f\"?@dataInicial='{start_str}'&@dataFinalCotacao='{end_str}'&$format=json\"\n",
    "        f\"&$select=cotacaoCompra,cotacaoVenda,dataHoraCotacao\"\n",
    "    )\n",
    "\n",
    "    data = _http_get(url)\n",
    "    j = json.loads(data.decode(\"utf-8\"))\n",
    "    values = j.get(\"value\", [])\n",
    "    df = pd.DataFrame(values)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"BCB PTAX: retorno vazio. Verifique janela de datas/URL.\")\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"dataHoraCotacao\"], errors=\"coerce\")\n",
    "    df[\"usdbrl\"] = pd.to_numeric(df[\"cotacaoVenda\"], errors=\"coerce\")\n",
    "\n",
    "    # Agregar para mensal (média)\n",
    "    df_m = (\n",
    "        df.dropna(subset=[\"date\", \"usdbrl\"])\n",
    "          .assign(date=_to_month_start(df[\"date\"]))\n",
    "          .groupby(\"date\", as_index=False)[\"usdbrl\"]\n",
    "          .mean()\n",
    "    )\n",
    "    return _month_index(df_m, \"date\")\n",
    "\n",
    "\n",
    "def load_nyfed_gscpi() -> pd.DataFrame:\n",
    "    content = _http_get(NYFED_GSCPI_XLSX)\n",
    "    xls = pd.ExcelFile(BytesIO(content))\n",
    "    # Em geral há duas sheet com a série e coluna \"GSCPI\"\n",
    "    sheet = xls.sheet_names[1]\n",
    "    df = pd.read_excel(xls, sheet_name=sheet)\n",
    "\n",
    "    # Tentar inferir colunas:\n",
    "    date_col = _pick_best_column(list(df.columns), [r\"date\", r\"month\", r\"time\"]) or df.columns[0]\n",
    "    val_col = _pick_best_column(list(df.columns), [r\"gscpi\"]) or df.columns[1]\n",
    "\n",
    "    out = df.rename(columns={date_col: \"date\", val_col: \"gscpi\"})[[\"date\", \"gscpi\"]].copy()\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "    out[\"gscpi\"] = pd.to_numeric(out[\"gscpi\"], errors=\"coerce\")\n",
    "    return _month_index(out, \"date\")\n",
    "\n",
    "\n",
    "def load_noaa_oni() -> pd.DataFrame:\n",
    "    content = _http_get(NOAA_ONI_CSV)\n",
    "    df = pd.read_csv(BytesIO(content))\n",
    "    # Esperado: date, ONI\n",
    "    date_col = _pick_best_column(list(df.columns), [r\"date\"]) or df.columns[0]\n",
    "    val_col = _pick_best_column(list(df.columns), [r\"oni\"]) or df.columns[1]\n",
    "    out = df.rename(columns={date_col: \"date\", val_col: \"oni\"})[[\"date\", \"oni\"]].copy()\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "    out[\"oni\"] = pd.to_numeric(out[\"oni\"], errors=\"coerce\")\n",
    "    return _month_index(out, \"date\")\n",
    "\n",
    "\n",
    "def load_gpr() -> pd.DataFrame:\n",
    "    content = _http_get(GPR_XLS)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(BytesIO(content), engine=\"xlrd\")\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"Para ler arquivos .xls, instale xlrd: pip install xlrd\"\n",
    "        ) from e\n",
    "    except ValueError:\n",
    "        # fallback: tenta sem engine explicita\n",
    "        df = pd.read_excel(BytesIO(content))\n",
    "\n",
    "    # Normaliza nomes\n",
    "    cols_norm = {c: re.sub(r\"\\s+\", \" \", str(c)).strip().lower() for c in df.columns}\n",
    "\n",
    "    # --- 1) Construir coluna date ---\n",
    "    # Caso A: tem year e month separados\n",
    "    year_col = next((c for c, cn in cols_norm.items() if cn in (\"year\", \"yr\", \"yyyy\")), None)\n",
    "    month_col = next((c for c, cn in cols_norm.items() if cn in (\"month\", \"mo\", \"mm\")), None)\n",
    "\n",
    "    if year_col is not None and month_col is not None:\n",
    "        out = df[[year_col, month_col]].copy()\n",
    "        out[\"date\"] = pd.to_datetime(\n",
    "            dict(year=pd.to_numeric(out[year_col], errors=\"coerce\"),\n",
    "                 month=pd.to_numeric(out[month_col], errors=\"coerce\"),\n",
    "                 day=1),\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "        df2 = df.copy()\n",
    "        df2[\"date\"] = out[\"date\"]\n",
    "    else:\n",
    "        # Caso B: tem date / time / month como coluna única\n",
    "        date_col = _pick_best_column(list(df.columns), [r\"date\", r\"time\", r\"month\"]) or df.columns[0]\n",
    "        df2 = df.rename(columns={date_col: \"date\"}).copy()\n",
    "        # Se vier como YYYYMM numérico (ex.: 202112), converte\n",
    "        if pd.api.types.is_numeric_dtype(df2[\"date\"]):\n",
    "            yyyymm = pd.to_numeric(df2[\"date\"], errors=\"coerce\")\n",
    "            year = (yyyymm // 100).astype(\"Int64\")\n",
    "            month = (yyyymm % 100).astype(\"Int64\")\n",
    "            df2[\"date\"] = pd.to_datetime(dict(year=year, month=month, day=1), errors=\"coerce\")\n",
    "        else:\n",
    "            df2[\"date\"] = pd.to_datetime(df2[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    # --- 2) Selecionar colunas do GPR ---\n",
    "    # Ele pode ter GPR agregado e também decomposições (ameaça/ato etc.). Vamos puxar tudo que começar com gpr\n",
    "    gpr_cols = [c for c, cn in cols_norm.items() if cn == \"gpr\" or cn.startswith(\"gpr\")]\n",
    "\n",
    "    if not gpr_cols:\n",
    "        # fallback: tenta achar pelo regex no nome original\n",
    "        gpr_cols = [c for c in df2.columns if re.search(r\"(^|\\W)gpr(\\W|$)\", str(c), flags=re.IGNORECASE)]\n",
    "\n",
    "    if not gpr_cols:\n",
    "        raise RuntimeError(\"Não encontrei nenhuma coluna 'GPR' no arquivo data_gpr_export.xls.\")\n",
    "\n",
    "    keep = [\"date\"] + gpr_cols\n",
    "    out = df2[keep].copy()\n",
    "\n",
    "    # Numeric\n",
    "    for c in gpr_cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "\n",
    "    # Padroniza nomes (opcional)\n",
    "    rename_map = {}\n",
    "    for c in gpr_cols:\n",
    "        cn = cols_norm.get(c, str(c).lower())\n",
    "        # deixa nomes amigáveis\n",
    "        rename_map[c] = re.sub(r\"[^a-z0-9_]+\", \"_\", cn).strip(\"_\")\n",
    "    out = out.rename(columns=rename_map)\n",
    "\n",
    "    return _month_index(out, \"date\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Feature engineering / EDA\n",
    "# =========================\n",
    "\n",
    "def add_seasonality(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    m = out.index.month.astype(int)\n",
    "    out[\"month\"] = m\n",
    "    out[\"month_sin\"] = np.sin(2 * np.pi * m / 12)\n",
    "    out[\"month_cos\"] = np.cos(2 * np.pi * m / 12)\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_lags(df: pd.DataFrame, cols: List[str], lags: List[int]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    new_cols = {}\n",
    "    for c in cols:\n",
    "        if c not in out.columns:\n",
    "            continue\n",
    "        for L in lags:\n",
    "            new_cols[f\"{c}_lag{L}\"] = out[c].shift(L)\n",
    "    if new_cols:\n",
    "        out = pd.concat([out, pd.DataFrame(new_cols)], axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_rolling(df: pd.DataFrame, cols: List[str], windows: List[int]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    new_cols = {}\n",
    "    for c in cols:\n",
    "        if c not in out.columns:\n",
    "            continue\n",
    "        for w in windows:\n",
    "            new_cols[f\"{c}_ma{w}\"] = out[c].rolling(w).mean()\n",
    "    if new_cols:\n",
    "        out = pd.concat([out, pd.DataFrame(new_cols)], axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def lag_correlation_table(df: pd.DataFrame, target: str, features: List[str], lags: List[int]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    y = df[target]\n",
    "    for f in features:\n",
    "        if f not in df.columns:\n",
    "            continue\n",
    "        for L in lags:\n",
    "            corr = y.corr(df[f].shift(L))\n",
    "            rows.append({\"feature\": f, \"lag\": L, \"corr\": corr})\n",
    "    out = pd.DataFrame(rows).dropna()\n",
    "    out[\"abs_corr\"] = out[\"corr\"].abs()\n",
    "    return out.sort_values([\"abs_corr\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def train_feature_importance_ts(\n",
    "    df: pd.DataFrame,\n",
    "    target: str,\n",
    "    drop_cols: Optional[List[str]] = None,\n",
    "    n_splits: int = 5,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "    drop_cols = drop_cols or []\n",
    "    data = df.dropna(subset=[target]).copy()\n",
    "    X = data.drop(columns=[target] + drop_cols, errors=\"ignore\")\n",
    "    y = data[target].copy()\n",
    "\n",
    "    # Remover colunas não-numéricas\n",
    "    X = X.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    # Tirar linhas com NA após lags/rolling\n",
    "    mask = X.notna().all(axis=1) & y.notna()\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_metrics = {\"mae\": [], \"rmse\": [], \"r2\": []}\n",
    "\n",
    "    # Modelo simples e interpretável via importâncias + permutação\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Avaliação em folds e treino final no full (para importâncias)\n",
    "    for tr, te in tscv.split(X):\n",
    "        Xtr, Xte = X.iloc[tr], X.iloc[te]\n",
    "        ytr, yte = y.iloc[tr], y.iloc[te]\n",
    "        model.fit(Xtr, ytr)\n",
    "        pred = model.predict(Xte)\n",
    "        fold_metrics[\"mae\"].append(mean_absolute_error(yte, pred))\n",
    "        fold_metrics[\"rmse\"].append(np.sqrt(mean_squared_error(yte, pred)))\n",
    "        fold_metrics[\"r2\"].append(r2_score(yte, pred))\n",
    "\n",
    "    metrics_summary = {\n",
    "        \"mae_mean\": float(np.mean(fold_metrics[\"mae\"])),\n",
    "        \"rmse_mean\": float(np.mean(fold_metrics[\"rmse\"])),\n",
    "        \"r2_mean\": float(np.mean(fold_metrics[\"r2\"])),\n",
    "        \"n_obs\": int(len(X)),\n",
    "        \"n_features\": int(X.shape[1]),\n",
    "    }\n",
    "\n",
    "    # Treino final para importâncias\n",
    "    model.fit(X, y)\n",
    "\n",
    "    imp_rf = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "    perm = permutation_importance(model, X, y, n_repeats=15, random_state=random_state, n_jobs=-1)\n",
    "    imp_perm = pd.Series(perm.importances_mean, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"rf_importance\": imp_rf.reindex(X.columns).values,\n",
    "        \"perm_importance\": imp_perm.reindex(X.columns).values,\n",
    "    }).sort_values([\"perm_importance\", \"rf_importance\"], ascending=False)\n",
    "\n",
    "    return out.reset_index(drop=True), metrics_summary\n",
    "\n",
    "\n",
    "def plot_series(df: pd.DataFrame, cols: List[str], outpath: str, title: str) -> None:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            plt.plot(df.index, df[c], label=c)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_bar(df: pd.DataFrame, x: str, y: str, outpath: str, title: str, top_n: int = 20) -> None:\n",
    "    d = df.head(top_n).copy()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(d[x][::-1], d[y][::-1])\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath, dpi=140)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = os.path.abspath(\"output\")\n",
    "START_DATE = \"2000-01\"\n",
    "END_DATE = \"2025-12\"\n",
    "TARGET = \"Urea  ($/mt)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c52cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_safe_mkdir(OUTDIR)\n",
    "figdir = os.path.join(OUTDIR, \"figures\")\n",
    "_safe_mkdir(figdir)\n",
    "\n",
    "# 1) Loaders\n",
    "print(\"Baixando Pink Sheet (World Bank)...\")\n",
    "df_prices = load_pink_sheet_monthly(PINK_SHEET_SERIES)\n",
    "\n",
    "print(\"Baixando câmbio PTAX (BCB)...\")\n",
    "df_fx = load_bcb_ptax_usdbrl(START_DATE, END_DATE)\n",
    "\n",
    "print(\"Baixando GSCPI (NY Fed)...\")\n",
    "df_gscpi = load_nyfed_gscpi()\n",
    "\n",
    "print(\"Baixando GPR (Geopolitical Risk)...\")\n",
    "df_gpr = load_gpr()\n",
    "\n",
    "print(\"Baixando ONI (NOAA)...\")\n",
    "df_oni = load_noaa_oni()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93cb7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2100cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Merge mensal\n",
    "df = df_prices.join(df_fx, how=\"outer\").join(df_gscpi, how=\"outer\").join(df_gpr, how=\"outer\").join(df_oni, how=\"outer\")\n",
    "df = df.loc[(df.index >= pd.to_datetime(START_DATE + \"-01\" if len(START_DATE) == 7 else START_DATE)) &\n",
    "            (df.index <= pd.to_datetime(END_DATE + \"-01\" if len(END_DATE) == 7 else END_DATE) + pd.offsets.MonthEnd(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0d782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: preço de ureia em BRL como feature/target alternativo\n",
    "if \"urea_usd\" in df.columns and \"usdbrl\" in df.columns:\n",
    "    df[\"urea_brl\"] = df[\"urea_usd\"] * df[\"usdbrl\"]\n",
    "\n",
    "# 3) Feature engineering\n",
    "df = add_seasonality(df)\n",
    "\n",
    "base_features = [c for c in df.columns if c not in {TARGET}]\n",
    "base_features = [c for c in base_features if pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "df = add_lags(df, cols=base_features, lags=[1, 2, 3, 6, 12])\n",
    "df = add_rolling(df, cols=base_features, windows=[3, 6, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) EDA: correlação com lags (tabela)\n",
    "feature_cols = [c for c in df.columns if c not in {TARGET}]\n",
    "lag_table = lag_correlation_table(df, target=TARGET, features=feature_cols, lags=[0, 1, 2, 3, 6, 12])\n",
    "lag_table.to_csv(os.path.join(OUTDIR, \"top_correlacoes.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cfe753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Modelo simples p/ ranking de importância (TimeSeriesSplit)\n",
    "importance_df, metrics = train_feature_importance_ts(\n",
    "    df=df,\n",
    "    target=TARGET,\n",
    "    drop_cols=[],\n",
    "    n_splits=5,\n",
    ")\n",
    "importance_df.to_csv(os.path.join(OUTDIR, \"feature_importance.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) EDA: correlação com lags (tabela)\n",
    "feature_cols = [c for c in df.columns if c not in {TARGET}]\n",
    "lag_table = lag_correlation_table(df, target=TARGET, features=feature_cols, lags=[0, 1, 2, 3, 6, 12])\n",
    "lag_table.to_csv(os.path.join(OUTDIR, \"top_correlacoes.csv\"), index=False)\n",
    "\n",
    "# 5) Modelo simples p/ ranking de importância (TimeSeriesSplit)\n",
    "importance_df, metrics = train_feature_importance_ts(\n",
    "    df=df,\n",
    "    target=TARGET,\n",
    "    drop_cols=[],\n",
    "    n_splits=5,\n",
    ")\n",
    "importance_df.to_csv(os.path.join(OUTDIR, \"feature_importance.csv\"), index=False)\n",
    "\n",
    "# 6) Salvar dataset final\n",
    "df_out = df.reset_index().rename(columns={\"index\": \"date\"})\n",
    "df_out.to_csv(os.path.join(OUTDIR, \"dataset_mensal.csv\"), index=False)\n",
    "\n",
    "# 7) Gráficos principais\n",
    "plot_series(df, cols=[TARGET], outpath=os.path.join(figdir, \"01_target.png\"), title=f\"Target: {TARGET}\")\n",
    "\n",
    "plot_series(\n",
    "    df,\n",
    "    cols=[c for c in [\"natural_gas_usd\", \"crude_oil_usd\", \"maize_usd\", \"wheat_usd\", \"usdbrl\", \"gscpi\", \"gpr\", \"oni\"] if c in df.columns],\n",
    "    outpath=os.path.join(figdir, \"02_principais_drivers.png\"),\n",
    "    title=\"Drivers (nível) - seleção\",\n",
    ")\n",
    "\n",
    "plot_bar(\n",
    "    importance_df,\n",
    "    x=\"feature\",\n",
    "    y=\"perm_importance\",\n",
    "    outpath=os.path.join(figdir, \"03_importancia_permutacao.png\"),\n",
    "    title=\"Importância (Permutation Importance) - Top 20\",\n",
    "    top_n=20,\n",
    ")\n",
    "\n",
    "# 8) Log de métricas\n",
    "with open(os.path.join(OUTDIR, \"metrics.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for k, v in metrics.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "print(\"\\nOK! Saídas em:\", OUTDIR)\n",
    "print(\"Métricas (CV):\", metrics)\n",
    "print(\"Dica: se quiser prever preço futuro, você pode trocar o target para 'urea_brl' e/ou criar target shift(-h).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urea-pricing-analysis-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
